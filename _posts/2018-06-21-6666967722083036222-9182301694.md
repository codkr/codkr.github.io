---                
layout: post       
title: Spark-yarn中save报错           
description: '<div class="content"></br><p class="">一、需求描述</br><br/>核心&amp;唯一需求：处理Spark on yarn（client）中saveAsHadoopDataset时报错</br><br/>系统环境：linux、hadoop2.6.0、spark1.6.1,5台集群</br><br/>目前状况：</br><br/>1：经将代码进行逐行注释测试，可以明确就是saveAsHadoopDataset时报错</br><br/>2：通过测试spark的自带示例，均可正常运行，可以推断基础环境正常</br><br/>3：参与运算及需要保存的数据量均在300M~600M之间</br><br/>二、相关关键信息</br><br/>1：代码（通过命令行调用）</br><br/>			// 从HDFS中读取数据</br><br/>			JavaPairRDD myResultRDDFinal = getPairRDDAgainstHDFS(ctx,strSourceHDFS);</br><br/></br><br/>			JavaPairRDD myReplacedRDD = myResultRDDFinal.flatMapToPair(new getFiltedRowkeyValueSetHDFS(allStartStopPairs1, lstCombinDimenMap));</br><br/>			JavaPairRDD newPureJavaPairRDD =  myReplacedRDD.filter(new getPureTuple());</br><br/>			// 进行RDD分组转换</br><br/>			JavaPairRDD&gt; myGroupedRDD = newPureJavaPairRDD.groupByKey();</br><br/>//			myGroupedRDD.</br><br/>			myGroupedRDD.persist(StorageLevel.MEMORY_ONLY_SER());</br><br/>			System.out.println("---------myGroupedRDD is persisted--------------------------------");</br><br/>			if(hdfsOrNot){ //存入HDFS</br><br/>				// 对分组后RDD求和yunsuan</br><br/>				JavaPairRDD allTuple2 = myGroupedRDD.mapToPair(new getNewTupleHDFS());</br><br/>				// 删除原有HDFS文件[WD]</br><br/>				rmDirRecursive(strWdHDFS);</br><br/>				// 将计算结果存入HDFS文件【以原有HDFS文件名称进行命名】</br><br/>				allTuple2.saveAsNewAPIHadoopFile(strWdHDFS,</br><br/>			    	Text.class, </br><br/>			    	Text.class, </br><br/>			    	TextOutputFormat.class,</br><br/>			    	getHadoopConf()</br><br/>			    );</br><br/></br><br/>			}else{//存入HBASE</br><br/>				JavaPairRDD allNewPut = myGroupedRDD.mapToPair(new getNewPut1(new Tuple2(strSrcHbaseColumnFamilyName,strSrcHbaseColumnName)));</br><br/>				allNewPut.persist(StorageLevel.MEMORY_ONLY_SER());</br><br/>				System.out.println("---------allNewPut is persisted--------------------------------");</br><br/>				JobConf jobStoreConfig  = new JobConf(conf, HbaseDimensionCalculate.class);</br><br/>				jobStoreConfig.setOutputFormat(TableOutputFormat.class);</br><br/>				jobStoreConfig.set(TableOutputFormat.OUTPUT_TABLE, strSrcHbaseTableName);</br><br/></br><br/>				allNewPut.saveAsHadoopDataset(jobStoreConfig);</br><br/>			}</br><br/>2：关键错误信息</br><br/>Container exited with a non-zero exit code 2</p></br><p class="">18/06/20 18:21:27 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1529060301846_0003_01_000006 on host: bigdata01. Exit status: 2. Diagnostics: Exception from container-launch.</br><br/>Container id: container_1529060301846_0003_01_000006</br><br/>Exit code: 2</br><br/>Stack trace: ExitCodeException exitCode=2: </br><br/>        at org.apache.hadoop.util.Shell.runCommand(Shell.java:538)</br><br/>        at org.apache.hadoop.util.Shell.run(Shell.java:455)</br><br/>        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)</br><br/>        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:211)</br><br/>        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)</br><br/>        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)</br><br/>        at java.util.concurrent.FutureTask.run(FutureTask.java:266)</br><br/>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)</br><br/>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)</br><br/>        at java.lang.Thread.run(Thread.java:745)</p></br><p class="">Container exited with a non-zero exit code 2</p></br><p class="">18/06/20 18:21:27 INFO YarnClientSchedulerBackend: Asked to remove non-existent executor 5</br><br/>18/06/20 18:21:27 INFO TaskSetManager: Starting task 0.1 in stage 1.3 (TID 11, bigdata01, partition 0,NODE_LOCAL, 2229 bytes)</br><br/>18/06/20 18:21:27 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on bigdata01:56463 (size: 42.2 KB, free: 5.2 GB)</br><br/>18/06/20 18:21:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to bigdata01:57735</br><br/>18/06/20 18:21:27 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 84 bytes</br><br/>18/06/20 18:21:27 WARN TaskSetManager: Lost task 0.1 in stage 1.3 (TID 11, bigdata01): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=0, message=</br><br/>org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0</br><br/>        at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:542)</br><br/>        at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:538)</br><br/>        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)</br><br/>        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)</br><br/>        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)</br><br/>        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)</br><br/>        at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:538)</br><br/>        at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:155)</br><br/>        at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:47)</br><br/>        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)</br><br/>        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)</br><br/>        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)</br><br/>        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)</br><br/>        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)</br><br/>        at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)</br><br/>        at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)</br><br/>        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)</br><br/>        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)</br><br/>        at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)</br><br/>        at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)</br><br/>        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)</br><br/>        at org.apache.spark.scheduler.Task.run(Task.scala:89)</br><br/>        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)</br><br/>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)</br><br/>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)</br><br/>        at java.lang.Thread.run(Thread.java:745)</p></br><p class="">)</br><br/>18/06/20 18:21:27 INFO DAGScheduler: Marking ResultStage 1 (saveAsHadoopDataset at HbaseDimensionCalculate.java:742) as failed due to a fetch failure from ShuffleMapStage 0 (filter at HbaseDimensionCalculate.java:715)</br><br/>18/06/20 18:21:27 INFO YarnScheduler: Removed TaskSet 1.3, whose tasks have all completed, from pool </br><br/>18/06/20 18:21:27 INFO DAGScheduler: ResultStage 1 (saveAsHadoopDataset at HbaseDimensionCalculate.java:742) failed in 3.856 s</br><br/>18/06/20 18:21:27 INFO DAGScheduler: Job 0 failed: saveAsHadoopDataset at HbaseDimensionCalculate.java:742, took 33.365694 s</br><br/>------------------llch Spark Exception in HbaseDimensionCalculate function:  Job aborted due to stage failure: ResultStage 1 (saveAsHadoopDataset at HbaseDimensionCalculate.java:742) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0</br><br/>        at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:542)</br><br/>        at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:538)</br><br/>        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)</br><br/>        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)</br><br/>        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)</br><br/>        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)</br><br/>        at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:538)</br><br/>        at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:155)</br><br/>        at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:47)</br><br/>        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)</br><br/>        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)</br><br/>        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)</br><br/>        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)</br><br/>        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)</br><br/>        at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)</br><br/>        at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)</br><br/>        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)</br><br/>        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)</br><br/>        at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)</br><br/>        at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)</br><br/>        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)</br><br/>        at org.apache.spark.scheduler.Task.run(Task.scala:89)</br><br/>        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)</br><br/>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)</br><br/>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)</br><br/>        at java.lang.Thread.run(Thread.java:745)</br><br/>3：spark配置信息</br><br/>spark.yarn.jar      hdfs:///myJar/spark-assembly-1.6.1-hadoop2.6.0.jar</br><br/>spark.driver.extraJavaOptions -XX:PermSize=128M -XX:MaxPermSize=256M </br><br/>spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"</br><br/>spark.driver.memory 3g</br><br/># spark.executor.memory 4g</br><br/>spark.driver.maxResultSize 3g</br><br/>spark.executorEnv.R_HOME  /home/local/bigdata/R-3.0.1/lib64/R</br><br/>spark.yarn.queue default</br><br/>spark.yarn.am.memory  1g</br><br/>spark.yarn.am.cores  1</br><br/>spark.executor.instances 3</br><br/>spark.executor.memory 8g</br><br/>spark.yarn.appMasterEnv.R_HOME  /home/local/bigdata/R-3.0.1/lib64/R</br><br/>spark.executorEnv.R_HOME  /home/local/bigdata/R-3.0.1/lib64/R</p></br><p class="">spark.default.parallelism 1</p></br><p class=""># spark.yarn.jar hdfs:/myJar/spark-assembly-1.6.1-hadoop2.6.0.jar</p></br><p class="">###0216</br><br/>spark.shuffle.file.buffer  64k</br><br/>spark.reducer.maxSizeInFlight  96m</br><br/>spark.shuffle.io.maxRetries  200</br><br/>spark.shuffle.io.retryWait  60s</br><br/>spark.shuffle.memoryFraction  0.5</br><br/>spark.shuffle.manager  sort</br><br/>spark.shuffle.consolidateFiles  true</br><br/>spark.shuffle.sort.bypassMergeThreshold  40		</br><br/>spark.shuffle.io.numConnectionsPerPeer  4</p></br><p class="">spark.network.timeout  1200s</br><br/>spark.rpc.lookupTimeout 1200s </br><br/>spark.shuffle.io.connectionTimeout 1200s</p></br><p class="">spark.task.maxFailures    		 10   </br><br/>#spark.task.cpus             	 2</br><br/>spark.task.cpus             	 1</br><br/>spark.speculation          		 true</br><br/>spark.blacklist.enabled    		 true</br><br/>spark.blacklist.task.maxTaskAttemptsPerExecutor 1</br><br/>spark.blacklist.task.maxTaskAttemptsPerNode   2</br><br/>spark.blacklist.stage.maxFailedTasksPerExecutor 2</br><br/>spark.blacklist.stage.maxFailedExecutorsPerNode 2</br><br/>spark.memory.useLegacyMode false</br><br/>spark.storage.memoryFraction   0.5</br><br/>spark.yarn.executor.memoryOverhead 1024</br><br/>三、人才要求</br><br/>2年以上hadoop 及spark架构设计开发实战经验</br><br/>四、其他补充说明</br><br/>1、不限坐班时间，如需要来现场，随时可接待</br><br/>2、因为是解决单个问题，所以只以结果作为付费依据</p></br></div>'     
contenturl: https://shixian.com/jobs/9182301694      
tags: [数据挖掘/爬虫,远程]            
salary: 预估1000元          
publish_time: '已发布 14 小时'         
start_time: '2018-06-21 10:15:02'           
apply: 0                   
time_range: 1天              
status: 招募中                  
---                 
